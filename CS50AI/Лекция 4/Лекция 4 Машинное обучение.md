# Лекция 4 Машинное обучение

Машинное обучение предоставляет компьютеру данные, а не явные инструкции. Используя эти данные, компьютер учится распознавать закономерности и становится способным выполнять задачи самостоятельно.

## Обучение с учителем

**Обучение с учителем** (Supervised learning) — это задача, в которой компьютер изучает функцию, сопоставляющую входы выходам на основе набора данных, состоящего из пар «вход-выход».

Существует несколько задач в рамках обучения с учителем, и одна из них — **Классификация** (Classification). Это задача, в которой функция сопоставляет входные данные дискретному выходу. Например, имея некоторую информацию о влажности и давлении воздуха за определенный день (вход), компьютер решает, будет ли в этот день дождь или нет (выход). Компьютер делает это после обучения на наборе данных, где для многих дней влажность и давление уже сопоставлены с тем, был дождь или нет.

Эту задачу можно формализовать следующим образом. Мы наблюдаем природу, где функция f({влажность, давление}) сопоставляет входные данные дискретному значению: «Дождь» или «Нет дождя». Эта функция f скрыта от нас и, вероятно, на нее влияют многие другие переменные, к которым у нас нет доступа. Наша цель — создать функцию h({влажность, давление}), которая сможет аппроксимировать (приблизить) поведение функции f. Такую задачу можно визуализировать, нанеся дни на график с осями влажности и давления (вход), окрасив каждую точку данных в синий цвет, если в этот день шел дождь, и в красный, если дождя не было (выход). Белая точка данных имеет только входные данные, и компьютеру необходимо определить выход.


![image](./.assets/image-1761418596057.png)


### Классификация по методу ближайшего соседа

Один из способов решения такой задачи — присвоить рассматриваемой переменной значение ближайшего к ней наблюдения. Так, например, белая точка на графике выше была бы окрашена в синий, потому что ближайшая наблюдаемая точка тоже синяя. Иногда это может работать хорошо, но рассмотрим график ниже.

![image](./.assets/image-1761418607624.png)

Следуя той же стратегии, белая точка должна быть окрашена в красный, поскольку ближайшее к ней наблюдение — красное. Однако, если посмотреть на картину в целом, кажется, что большинство других наблюдений вокруг нее — синие, что может подсказать нам, что синий цвет является лучшим прогнозом в этом случае, несмотря на то, что ближайшее наблюдение — красное.

Один из способов обойти ограничения классификации по ближайшему соседу — использовать **классификацию по k-ближайшим соседям (k-nearest-neighbors)**, где точка окрашивается в цвет, наиболее часто встречающийся среди $k$ ближайших соседей. Программист сам решает, чему равно $k$. Используя, например, классификацию по 3-м ближайшим соседям, белая точка выше будет окрашена в синий, что интуитивно кажется лучшим решением.

Недостатком классификации по k-ближайшим соседям является то, что при наивном подходе алгоритму придется измерять расстояние от каждой точки до рассматриваемой, что является вычислительно затратным. Это можно ускорить, используя структуры данных, позволяющие быстрее находить соседей, или отсекая (pruning) нерелевантные наблюдения.



### Обучение перцептрона

Другой подход к задаче классификации, в отличие от стратегии ближайшего соседа, — это посмотреть на данные в целом и попытаться построить **границу принятия решений** (decision boundary). В двумерных данных мы можем провести линию между двумя типами наблюдений. Каждая дополнительная точка данных будет классифицироваться в зависимости от того, по какую сторону линии она находится.

![image](./.assets/image-1761418615601.png)

Недостаток этого подхода в том, что данные «грязные», и редко когда можно провести линию и аккуратно разделить классы на два наблюдения без ошибок. Часто нам приходится идти на компромисс, проводя границу, которая разделяет наблюдения правильно в большинстве случаев, но все же иногда допускает ошибки в классификации.

В этом случае входные данные:

x₁= Влажность

x₂= Давление

будут подаваться в функцию гипотезы (hypothesis function) h(x₁, x₂), которая будет выдавать свой прогноз, пойдет ли в этот день дождь. Она будет делать это, проверяя, по какую сторону от границы принятия решений находится наблюдение. Формально, функция будет взвешивать каждый из входов с добавлением константы, что в итоге дает линейное уравнение следующего вида:

Дождь, если w₀ + w₁x₁ + w₂x₂ ≥ 0

Нет дождя, в противном случае

Часто выходная переменная кодируется как 1 и 0, где если уравнение дает значение больше или равное 0, выход равен 1 (Дождь), и 0 в противном случае (Нет дождя).

Веса и значения представляются **векторами** (vectors), которые являются последовательностями чисел (в Python их можно хранить в списках или кортежах). Мы создаем **вектор весов** (Weight Vector) w: (w₀, w₁, w₂), и нахождение наилучшего вектора весов является целью алгоритма машинного обучения. Мы также создаем **вектор входа** (Input Vector) x: (1, x₁, x₂).

Мы берем **скалярное произведение (dot product)** двух векторов. То есть, мы умножаем каждое значение в одном векторе на соответствующее значение во втором векторе, получая выражение, указанное выше: w₀ + w₁x₁ + w₂x₂. Первое значение во входном векторе равно 1, потому что при умножении на весовой коэффициент w₀ мы хотим сохранить его как константу (смещение).

Таким образом, мы можем представить нашу функцию гипотезы следующим образом:


![image](./.assets/image-1761418628230.png)


Поскольку цель алгоритма — найти лучший вектор весов, при поступлении новых данных он обновляет текущие веса. Он делает это, используя **правило обучения перцептрона**:

![image](./.assets/image-1761418636951.png)

Важный вывод из этого правила заключается в том, что для каждой точки данных мы корректируем веса, чтобы сделать нашу функцию более точной. Детали, которые не так важны для нашей точки зрения, заключаются в том, что каждый вес устанавливается равным самому себе плюс некоторое значение в скобках. Здесь y обозначает наблюдаемое значение, а функция гипотезы — оценку. Если они идентичны, то весь этот член равен нулю, и таким образом вес не меняется. Если мы недооценили (вызвав отсутствие дождя, пока наблюдался дождь), то значение в скобках будет равно 1 и вес увеличится на величину xi, масштабированную на α коэффициента обучения. Если мы переоценили (вызвав Rain, когда дождя не наблюдалось), то значение в скобках будет -1 и вес уменьшится на значение x, масштабированное на α. Чем выше α, тем сильнее влияние каждого нового события на вес.

Результатом этого процесса является функция порога, которая переключается с 0 на 1, как только оценочное значение пересекает некоторый порог.

![image](./.assets/image-1761418642297.png)

Проблема этого типа функции в том, что она неспособна выразить неопределенность, поскольку может принимать значения только 0 или 1. Она использует **жесткий порог** (hard threshold). Способ обойти это — использовать **логистическую функцию** (logistic function), которая применяет **мягкий порог** (soft threshold). Логистическая функция может выдавать вещественное число от 0 до 1, которое будет выражать уверенность в оценке. Чем ближе значение к 1, тем выше вероятность дождя.

![image](./.assets/image-1761418646081.png)

### Метод опорных векторов

В дополнение к методу ближайшего соседа и линейной классификации (в тексте лекции ошибочно указана "линейная регрессия"), другим подходом к классификации является **Метод Опорных Векторов** (Support Vector Machine, SVM). Этот подход использует дополнительный вектор (опорный вектор) вблизи границы принятия решений, чтобы принять наилучшее решение при разделении данных. Рассмотрим пример ниже.

![image](./.assets/image-1761418651073.png)

Все границы принятия решений на рисунке работают, поскольку они разделяют данные без ошибок. Однако, одинаково ли они хороши? Две левые границы принятия решений находятся очень близко к некоторым наблюдениям. Это означает, что новая точка данных, лишь незначительно отличающаяся от одной группы, может быть ошибочно классифицирована как другая. В отличие от них, самая правая граница принятия решений сохраняет наибольшее расстояние от каждой из групп, тем самым предоставляя наибольший «зазор» для вариаций внутри группы. Такой тип границы, который находится как можно дальше от двух разделяемых групп, называется **Разделителем с Максимальным Зазором** (Maximum Margin Separator).

Еще одно преимущество методов опорных векторов заключается в том, что они могут представлять границы принятия решений с более чем двумя измерениями, а также нелинейные границы принятия решений, как показано ниже.

![image](./.assets/image-1761418656645.png)

Подводя итог, существует несколько способов решения задач классификации, и ни один из них не является всегда лучшим. У каждого есть свои недостатки, и он может оказаться более полезным, чем другие, в конкретных ситуациях.

-----

## Регрессия

**Регрессия** (Regression) — это задача обучения с учителем, заключающаяся в обучении функции, которая сопоставляет входную точку непрерывному значению, некоторому вещественному числу. Это отличает ее от классификации, где задачи сопоставляют входные данные дискретным значениям (Дождь или Нет дождя).

Например, компания может использовать регрессию для ответа на вопрос о том, как деньги, потраченные на рекламу, предсказывают деньги, заработанные на продажах. В этом случае наблюдаемая функция *f(advertising)* представляет собой наблюдаемый доход после того, как некоторые деньги были потрачены на рекламу (обратите внимание, что функция может принимать более одной входной переменной). Вот с этих данных мы и начинаем. Используя эти данные, мы хотим придумать гипотезу функции *h(advertising),* которая попытается аппроксимировать поведение функции *f*. *h* сгенерирует линию, цель которой состоит не в разделении между типами наблюдений, а в прогнозировании на основе входных данных, какова будет ценность выходных данных.

![image](./.assets/image-1761418662905.png)

-----

## Функции потерь

**Функции потерь** (Loss functions) — это способ количественно оценить «полезность», потерянную из-за любого из вышеперечисленных правил принятия решений. Чем менее точен прогноз, тем больше потери.

Для задач классификации мы можем использовать **Функцию потерь 0-1** (0-1 Loss Function).

L(факт, прогноз):
0, если факт = прогноз
1, в противном случае

Другими словами, эта функция «получает штраф» (увеличивает значение), когда прогноз неверен, и не получает его, когда он верен (т.е. когда наблюдаемые и предсказанные значения совпадают).

![image](./.assets/image-1761418668425.png)

В примере выше, дни, оцененные в 0, — это те, в которые мы правильно предсказали погоду (дождливые дни находятся под линией, а не дождливые — над линией). Однако дни, когда дождя не было, но они оказались под линией, и дни, когда дождь был, но они оказались над ней, — это те, которые мы предсказать не смогли. Мы присваиваем каждому из них значение 1 и суммируем их, чтобы получить эмпирическую оценку того, насколько «убыточна» наша граница принятия решений.

**Функции потерь L₁ и L₂** можно использовать при прогнозировании непрерывного значения. В этом случае нас интересует количественная оценка, **насколько** каждый прогноз отличался от наблюдаемого значения. Мы делаем это, беря либо абсолютное значение (модуль), либо квадрат разницы между наблюдаемым значением и предсказанным (т.е. насколько далеко прогноз отстоял от наблюдаемого значения).

* L₁: *L*(фактический, прогнозируемый) = |фактический - прогнозируемый|
* L₂: *L*(фактический, прогнозируемый) = (фактический - прогнозируемый)²

Можно выбрать ту функцию потерь, которая лучше всего соответствует целям. L₂ штрафует за «выбросы» (outliers) более сурово, чем L₁, поскольку она возводит разницу в квадрат. L₁ можно визуализировать, суммируя расстояния от каждой наблюдаемой точки до предсказанной точки на линии регрессии.

![image](./.assets/image-1761418672772.png)

-----

## Переобучение

**Переобучение** (Overfitting) — это ситуация, когда модель настолько хорошо подстраивается под обучающие данные, что неспособна **обобщаться** (generalize) на другие наборы данных. В этом смысле функции потерь — это палка о двух концах. В двух примерах ниже функция потерь минимизирована так, что потери равны 0. Однако маловероятно, что она будет хорошо работать на новых данных.

![image](./.assets/image-1761418677854.png)

Например, на левом графике точка рядом с красной в нижней части экрана, скорее всего, должна быть «Дождь» (синяя). Однако при переобученной модели она будет классифицирована как «Нет дождя» (красная).

-----

## Регуляризация

**Регуляризация** (Regularization) — это процесс «штрафования» гипотез, которые являются более сложными, в пользу более простых, более общих гипотез. Мы используем регуляризацию, чтобы избежать переобучения.

При регуляризации мы оцениваем **стоимость** (cost) функции гипотезы h, суммируя ее **потери** (loss) и меру ее **сложности** (complexity).

*стоимость*(h) = *потери*(h) + λ*сложность*(h)

Лямбда (λ) — это константа, которую мы можем использовать для модуляции того, насколько сильно наказывать за сложность в нашей функции стоимости. Чем выше λ, тем дороже сложность.

Один из способов проверить, не переобучили ли мы модель, — это **отложенная перекрестная проверка** (Holdout Cross Validation). В этом методе мы делим все данные на две части: **обучающий набор** (training set) и **тестовый набор** (test set). Мы запускаем алгоритм обучения на обучающем наборе, а затем смотрим, насколько хорошо он предсказывает данные в тестовом наборе. Идея здесь в том, что, тестируя на данных, которые не использовались при обучении, мы можем измерить, насколько хорошо модель обобщается.

Недостатком перекрестной проверки без исключения является то, что мы не можем обучить модель на половине данных, поскольку они используются для оценки. Одним из способов решения этой проблемы является использование ***k-Fold* Cross-Validation**. В этом процессе мы делим данные на k наборов. Мы запускаем обучение k раз, каждый раз опуская один набор данных и используя его в качестве тестового набора. В итоге мы получаем k различных оценок нашей модели, которые мы можем усреднить и получить оценку того, как наша модель обобщается без потери данных.

-----

## scikit-learn

Как это часто бывает с Python, существует множество библиотек, которые позволяют нам удобно использовать алгоритмы машинного обучения. Одной из таких библиотек является **scikit-learn**.

В качестве примера мы будем использовать набор данных в формате **CSV** о фальшивых банкнотах.

![image](./.assets/image-1761418687699.png)

Четыре левых столбца — это данные, которые мы можем использовать для прогнозирования, является ли банкнота подлинной или фальшивой (это внешние данные, предоставленные человеком, закодированные как 0 и 1). Теперь мы можем обучить нашу модель на этом наборе данных и посмотреть, сможем ли мы предсказать, являются ли новые банкноты подлинными или нет.

```python
import csv
import random

from sklearn import svm
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# model = KNeighborsClassifier(n_neighbors=1) # Модель = Классификатор K-ближайших соседей
# model = svm.SVC() # Модель = Метод опорных векторов
model = Perceptron() # Модель = Перцептрон
```

Обратите внимание, что после импорта библиотек мы можем выбрать, какую модель использовать. Остальная часть кода останется прежней. SVC означает Support Vector Classifier (Классификатор на основе опорных векторов, который мы знаем как метод опорных векторов). KNeighborsClassifier использует стратегию k-соседей и требует в качестве входных данных количество соседей, которое он должен учитывать.

```python
# Читаем данные из файла
with open("banknotes.csv") as f:
    reader = csv.reader(f)
    next(reader)

    data = []
    for row in reader:
        data.append({
            "evidence": [float(cell) for cell in row[:4]],
            "label": "Authentic" if row[4] == "0" else "Counterfeit" # "Подлинная" или "Фальшивая"
        })

# Разделяем данные на обучающую и тестовую группы
holdout = int(0.40 * len(data))
random.shuffle(data)
testing = data[:holdout]
training = data[holdout:]

# Обучаем модель на обучающем наборе
X_training = [row["evidence"] for row in training]
y_training = [row["label"] for row in training]
model.fit(X_training, y_training)

# Делаем предсказания на тестовом наборе
X_testing = [row["evidence"] for row in testing]
y_testing = [row["label"] for row in testing]
predictions = model.predict(X_testing)

# Вычисляем, насколько хорошо мы справились
correct = 0
incorrect = 0
total = 0
for actual, predicted in zip(y_testing, predictions):
    total += 1
    if actual == predicted:
        correct += 1
    else:
        incorrect += 1

# Выводим результаты
print(f"Результаты для модели {type(model).__name__}")
print(f"Правильно: {correct}")
print(f"Неправильно: {incorrect}")
print(f"Точность: {100 * correct / total:.2f}%")
```

Эта «ручная» версия запуска алгоритма находится в исходном коде этой лекции под названием `banknotes0.py`. Поскольку этот алгоритм часто используется похожим образом, `scikit-learn` содержит дополнительные функции, которые делают код еще более лаконичным и простым в использовании, и эту версию можно найти в `banknotes1.py`.

-----

## Обучение с подкреплением

**Обучение с подкреплением** (Reinforcement learning) — это еще один подход к машинному обучению, при котором после каждого действия **агент** (agent) получает обратную связь в виде **вознаграждения** (reward) или **наказания** (punishment) (положительное или отрицательное числовое значение).

![image](./.assets/image-1761418698210.png)

Процесс обучения начинается с того, что **среда** (environment) предоставляет агенту **состояние** (state). Затем агент выполняет **действие** (action) в этом состоянии. На основе этого действия среда вернет агенту новое состояние и вознаграждение, которое может быть положительным (что делает поведение более вероятным в будущем) или отрицательным (т.е. наказанием, делающим поведение менее вероятным).

Этот тип алгоритма можно использовать, например, для обучения шагающих роботов, где каждый шаг возвращает положительное число (вознаграждение), а каждое падение — отрицательное (наказание).

### Марковские процессы принятия решений

Обучение с подкреплением можно рассматривать как **Марковский процесс принятия решений** (Markov Decision Process), обладающий следующими свойствами:

* Множество состояний ***S***
* **Набор действий ***Action(S)***
* **Модель перехода ***P(s' | s, a)***
* **Функция вознаграждения ***R(s, a, s')***

Например, рассмотрим следующую задачу:

![image](./.assets/image-1761418703785.png)

Агент — это желтый круг, и ему нужно добраться до зеленого квадрата, избегая красных квадратов. Каждый отдельный квадрат в задаче — это состояние. Движение вверх, вниз или в стороны — это действие. Модель перехода дает нам новое состояние после выполнения действия, а функция вознаграждения — это та обратная связь, которую получает агент. Например, если агент решит пойти вправо, он наступит на красный квадрат и получит отрицательную обратную связь. Это означает, что агент научится, что, находясь в состоянии нижнего левого квадрата, следует избегать движения вправо. Таким образом, агент начнет исследовать пространство, изучая, каких пар «состояние-действие» ему следует избегать. Алгоритм может быть вероятностным, выбирая разные действия в разных состояниях на основе некоторой вероятности, которая увеличивается или уменьшается в зависимости от вознаграждения. Когда агент достигнет зеленого квадрата, он получит положительное вознаграждение, узнав, что действие, которое он предпринял в предыдущем состоянии, было благоприятным.

### Q-обучение

Q-обучение является одной из моделей обучения с подкреплением, где функция ***Q(s, a)*** выводит оценку ценности выполнения действия *a* в состоянии *s*.

Модель начинается со всех оценочных значений, равных 0 (***Q(s,a)* = 0** для всех *s, a*). Когда выполняется действие и получено вознаграждение, функция выполняет две вещи: 1) она оценивает значение ***Q(s, a)*** на основе текущего вознаграждения и ожидаемых будущих вознаграждений, и 2) обновляет ***Q(s, a),*** чтобы учесть как старую, так и новую оценку. Это дает нам алгоритм, способный улучшить свои прошлые знания, не начиная с нуля.

***Q(s, a) ⟵ Q(s, a) + α(оценка новой величины - Q(s, a))***

Обновленное значение ***Q(s, a)*** равно предыдущему значению ***Q(s, a)*** в дополнение к некоторому обновляемому значению. Эта величина определяется как разница между новым значением и старым значением, умноженная на α, коэффициент обучения. Когда α = 1, новая оценка просто перезаписывает старую. Когда α = 0, расчетное значение никогда не обновляется. Повышая и опуская α, мы можем определить, насколько быстро предыдущие знания обновляются новыми оценками.

Новая оценка стоимости может быть выражена в виде суммы вознаграждения (r) и оценки будущего вознаграждения. Чтобы получить оценку будущего вознаграждения, мы рассматриваем новое состояние, которое мы получили после совершения последнего действия, и прибавляем оценку действия в этом новом состоянии, которое принесет наибольшую награду. Таким образом, мы оцениваем полезность *совершения действия* в состоянии *не* только по полученному вознаграждению, но и по ожидаемой полезности следующего шага. Значение оценки будущего вознаграждения иногда может отображаться с помощью гаммы коэффициента, которая определяет, насколько будут оценены будущие вознаграждения. В итоге мы получаем следующее уравнение:


![image](./.assets/image-1761418716697.png)


Жадный (Greedy) алгоритм принятия решений полностью игнорирует будущие оценочные вознаграждения, вместо этого всегда выбирая то действие a в текущем состоянии s, которое имеет наивысшее значение Q(s, a).

Это подводит нас к обсуждению компромисса **Исследование vs. Эксплуатация** (Explore vs. Exploit). Жадный алгоритм всегда «эксплуатирует», выбирая действия, которые уже зарекомендовали себя как приносящие хороший результат. Однако он всегда будет следовать одним и тем же путем к решению, никогда не находя лучшего. «Исследование», с другой стороны, означает, что алгоритм может использовать ранее неизведанный маршрут на пути к цели, что позволяет ему находить более эффективные решения. Например, если вы каждый раз слушаете одни и те же песни, вы знаете, что они вам понравятся, но вы никогда не узнаете новые песни, которые могут вам понравиться еще больше!

Для реализации концепции разведки и эксплуатации мы можем использовать **жадный алгоритм ε (эпсилон).** В этом типе алгоритма мы устанавливаем ε равным тому, как часто мы хотим двигаться случайным образом. С вероятностью 1-ε алгоритм выбирает лучший ход (эксплуатацию). При ε вероятности алгоритм выбирает случайный ход (исследование).

Другой способ обучить модель с подкреплением — давать обратную связь не после каждого хода, а по завершении всего процесса. Например, рассмотрим игру Ним. В этой игре разное количество объектов распределено по кучкам. Каждый игрок берет любое количество объектов из любой одной кучки, и игрок, взявший последний объект, проигрывает. В такой игре необученный ИИ будет играть случайным образом, и у него будет легко выиграть. Чтобы обучить ИИ, он начнет играть в игру случайным образом, и в конце получит вознаграждение 1 за победу и -1 за поражение. Когда он обучится, например, на 10 000 играх, против него уже будет сложно выиграть.

Этот подход становится более вычислительно требовательным, когда в игре много состояний и возможных действий, как, например, в шахматах. Невозможно сгенерировать оценочное значение для каждого возможного хода в каждом возможном состоянии. В этом случае мы можем использовать **аппроксимацию функции** (function approximation), которая позволяет нам аппроксимировать Q(s, a) с использованием различных других **признаков** (features), вместо того чтобы хранить одно значение для каждой пары «состояние-действие». Таким образом, алгоритм становится способным распознавать, какие ходы достаточно похожи, чтобы их оценочная ценность также была схожей, и использовать эту эвристику при принятии решений.



## Обучение без учителя

Во всех случаях, которые мы рассматривали ранее, как и в обучении с учителем, у нас были данные с **метками** (labels), на которых алгоритм мог учиться. Например, когда мы обучали алгоритм распознавать фальшивые банкноты, у каждой банкноты было четыре переменные с разными значениями (входные данные) и метка, фальшивая она или нет. В **обучении без учителя** (unsupervised learning) присутствуют только входные данные, и ИИ изучает закономерности в этих данных.



### Кластеризация

**Кластеризация** (Clustering) — это задача обучения без учителя, которая берет входные данные и организует их в группы таким образом, чтобы похожие объекты оказались в одной группе. Это можно использовать, например, в генетических исследованиях при поиске схожих генов или в сегментации изображений при определении различных частей изображения на основе сходства между пикселями.



### Кластеризация k-средних

**Кластеризация k-средних** (k-means Clustering) — это алгоритм для выполнения задачи кластеризации. Он наносит все точки данных на пространство, а затем случайным образом размещает в пространстве k **центров кластеров** (cluster centers) (программист сам решает, сколько их будет; это начальное состояние, которое мы видим слева). Каждый центр кластера — это просто точка в пространстве. Затем каждому кластеру присваиваются все точки, которые находятся ближе к его центру, чем к любому другому (это средняя картинка). Затем в итеративном процессе центр кластера перемещается в середину всех этих точек (состояние справа), а затем точки снова переназначаются кластерам, центры которых теперь к ним ближе всего. Когда после повторения процесса каждая точка остается в том же кластере, что и раньше, мы достигли равновесия, и алгоритм завершен, оставляя нам точки, разделенные на кластеры.

![image](./.assets/image-1761418721965.png)







