# Решение задания 42
 
 nim.py
 
Этот код реализует класс `NimAI`, используя Q-learning для обучения ИИ игре в Ним. Реализованные функции управляют получением Q-значений, их обновлением по формуле Q-learning, расчетом будущих вознаграждений и выбором действий с использованием ε-greedy (эпсилон-жадного) подхода.

```python
import math
import random
import time

class Nim():

    def __init__(self, initial=[1, 3, 5, 7]):
        """
        Initialize game board.
        Each game board has
            - `piles`: a list of how many elements remain in each pile
            - `player`: 0 or 1 to indicate which player's turn
            - `winner`: None, 0, or 1 to indicate who the winner is
        """
        self.piles = initial.copy()
        self.player = 0
        self.winner = None

    @classmethod
    def available_actions(cls, piles):
        """
        Nim.available_actions(piles) takes a `piles` list as input
        and returns all of the available actions `(i, j)` in that state.

        Action `(i, j)` represents the action of removing `j` items
        from pile `i` (where piles are 0-indexed).
        """
        actions = set()
        for i, pile in enumerate(piles):
            for j in range(1, pile + 1):
                actions.add((i, j))
        return actions

    @classmethod
    def other_player(cls, player):
        """
        Nim.other_player(player) returns the player that is not
        `player`. Assumes `player` is either 0 or 1.
        """
        return 0 if player == 1 else 1

    def switch_player(self):
        """
        Switch the current player to the other player.
        """
        self.player = Nim.other_player(self.player)

    def move(self, action):
        """
        Make the move `action` for the current player.
        `action` must be a tuple `(i, j)`.
        """
        pile, count = action

        # Check for errors
        if self.winner is not None:
            raise Exception("Game already won")
        elif pile < 0 or pile >= len(self.piles):
            raise Exception("Invalid pile")
        elif count < 1 or count > self.piles[pile]:
            raise Exception("Invalid number of objects")

        # Update pile
        self.piles[pile] -= count
        self.switch_player()

        # Check for a winner
        if all(pile == 0 for pile in self.piles):
            self.winner = self.player


class NimAI():

    def __init__(self, alpha=0.5, epsilon=0.1):
        """
        Initialize AI with an empty Q-learning dictionary,
        an alpha (learning) rate, and an epsilon rate.

        The Q-learning dictionary maps `(state, action)`
        pairs to a Q-value (a number).
         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)
         - `action` is a tuple `(i, j)` for an action
        """
        self.q = dict()
        self.alpha = alpha
        self.epsilon = epsilon

    def update(self, old_state, action, new_state, reward):
        """
        Update Q-learning model, given an old state, an action taken
        in that state, a new resulting state, and the reward received
        from taking that action.
        """
        old = self.get_q_value(old_state, action)
        best_future = self.best_future_reward(new_state)
        self.update_q_value(old_state, action, old, reward, best_future)

    def get_q_value(self, state, action):
        """
        Return the Q-value for the state `state` and the action `action`.
        If no Q-value exists yet in `self.q`, return 0.
        """
        # Используем tuple(state) в качестве ключа, как указано [cite: 98, 110]
        # .get() возвращает 0, если ключ (state, action) не найден [cite: 111]
        return self.q.get((tuple(state), action), 0)

    def update_q_value(self, state, action, old_q, reward, future_rewards):
        """
        Update the Q-value for the state `state` and the action `action`
        given the previous Q-value `old_q`, a current reward `reward`,
        and an estiamte of future rewards `future_rewards`.

        Use the formula:

        Q(s, a) <- old value estimate
                   + alpha * (new value estimate - old value estimate)

        where `old value estimate` is the previous Q-value,
        `alpha` is the learning rate, and `new value estimate`
        is the sum of the current reward and estimated future rewards.
        """
        # old value estimate = old_q [cite: 113]
        # new value estimate = reward + future_rewards [cite: 114]
        # alpha = self.alpha [cite: 113]
        
        new_value_estimate = reward + future_rewards
        updated_q = old_q + self.alpha * (new_value_estimate - old_q)
        
        # Сохраняем обновленное Q-значение в словаре [cite: 112]
        self.q[(tuple(state), action)] = updated_q


    def best_future_reward(self, state):
        """
        Given a state `state`, consider all possible `(state, action)`
        pairs available in that state and return the maximum of all
        of their Q-values.

        Use 0 as the Q-value if a `(state, action)` pair has no
        Q-value in `self.q`. If there are no available actions in
        `state`, return 0.
        """
        # Получаем все доступные действия для данного состояния [cite: 93]
        available_actions = Nim.available_actions(state)
        
        # Если нет доступных действий, возвращаем 0 [cite: 116]
        if not available_actions:
            return 0
        
        # Находим максимальное Q-значение среди всех доступных действий
        # get_q_value вернет 0 для неизвестных действий [cite: 116]
        max_q_value = max(
            self.get_q_value(state, action) for action in available_actions
        )
        return max_q_value

    def choose_action(self, state, epsilon=True):
        """
        Given a state `state`, return an action `(i, j)` to take.

        If `epsilon` is `False`, then return the best action
        available in the state (the one with the highest Q-value,
        using 0 for pairs that have no Q-values). [cite: 117]

        If `epsilon` is `True`, then with probability
        `self.epsilon` choose a random available action,
        otherwise choose the best action available. 

        If multiple actions have the same Q-value, any of those
        options is an acceptable return value. [cite: 119]
        """
        available_actions = Nim.available_actions(state)
        
        # Epsilon-greedy (исследование) 
        if epsilon and random.random() < self.epsilon:
            # Выбираем случайное действие
            return random.choice(list(available_actions))
        
        # Greedy (эксплуатация) или epsilon=False [cite: 117]
        else:
            # Находим действие с максимальным Q-значением
            # Используем lambda-функцию для получения Q-значения каждого действия
            best_action = max(
                available_actions, 
                key=lambda action: self.get_q_value(state, action)
            )
            return best_action


def train(n):
    """
    Train an AI by playing `n` games against itself.
    """

    player = NimAI()

    # Play n games
    for i in range(n):
        print(f"Playing training game {i + 1}")
        game = Nim()

        # Keep track of last move made by either player
        last = {
            0: {"state": None, "action": None},
            1: {"state": None, "action": None}
        }

        # Game loop
        while True:

            # Keep track of current state and action
            state = game.piles.copy()
            action = player.choose_action(game.piles)

            # Keep track of last state and action
            last[game.player]["state"] = state
            last[game.player]["action"] = action

            # Make move
            game.move(action)
            new_state = game.piles.copy()

            # When game is over, update Q values with rewards
            if game.winner is not None:
                # Победивший игрок (теперь game.player) получает -1 за свой *последний* ход (который привел к проигрышу)
                player.update(state, action, new_state, -1)
                # Проигравший игрок (last[game.player]) получает +1 за свой *предпоследний* ход (который привел к победе)
                player.update(
                    last[game.player]["state"],
                    last[game.player]["action"],
                    new_state,
                    1
                )
                break

            # If game is continuing, no rewards yet
            # Обновляем Q-значение для *предыдущего* хода *другого* игрока (теперь game.player)
            elif last[game.player]["state"] is not None:
                player.update(
                    last[game.player]["state"],
                    last[game.player]["action"],
                    new_state,
                    0
                )

    print("Done training")

    # Return the trained AI
    return player


def play(ai, human_player=None):
    """
    Play human game against the AI.
    `human_player` can be set to 0 or 1 to specify whether
    human player moves first or second.
    """

    # If no player order set, choose human's order randomly
    if human_player is None:
        human_player = random.randint(0, 1)

    # Create new game
    game = Nim()

    # Game loop
    while True:

        # Print contents of piles
        print()
        print("Piles:")
        for i, pile in enumerate(game.piles):
            print(f"Pile {i}: {pile}")
        print()

        # Compute available actions
        available_actions = Nim.available_actions(game.piles)
        time.sleep(1)

        # Let human make a move
        if game.player == human_player:
            print("Your Turn")
            while True:
                try:
                    pile = int(input("Choose Pile: "))
                    count = int(input("Choose Count: "))
                    if (pile, count) in available_actions:
                        break
                    print("Invalid move, try again.")
                except ValueError:
                    print("Invalid input, please enter numbers.")

        # Have AI make a move
        else:
            print("AI's Turn")
            # ИИ всегда выбирает лучший ход (epsilon=False) против человека [cite: 117]
            pile, count = ai.choose_action(game.piles, epsilon=False)
            print(f"AI chose to take {count} from pile {pile}.")

        # Make move
        game.move((pile, count))

        # Check for winner
        if game.winner is not None:
            print()
            print("GAME OVER")
            winner = "Human" if game.winner == human_player else "AI"
            print(f"Winner is {winner}")
            return
```



## [Фон]
Напомним, что в игре Nim мы начинаем с некоторого количества стопок, в каждой из которых находится какое-то количество объектов. Игроки ходят по очереди: в свой ход игрок убирает любое неотрицательное количество объектов из любой непустой кучи. Тот, кто уберет последний объект, проигрывает.

Есть несколько простых стратегий, которые вы можете придумать для этой игры: если в ней осталась только одна куча и три объекта, и настала ваша очередь, лучше всего убрать два из этих объектов, оставив вашему противнику третий и последний объект, который нужно убрать. Но если свай больше, стратегия значительно усложняется. В этой задаче мы создадим искусственный интеллект, который будет обучаться стратегии для этой игры с помощью обучения с подкреплением. Постоянно играя сам с собой и учась на опыте, в конечном итоге наш ИИ узнает, какие действия следует предпринять, а каких избегать.

В частности, для этого проекта мы будем использовать Q-обучение. Вспомним, что в Q-обучении мы пытаемся узнать значение вознаграждения (число) для каждой пары. Действие, которое проигрывает игру, будет иметь награду -1, действие, которое приводит к тому, что другой игрок проигрывает игру, будет иметь награду 1, а действие, которое приводит к продолжению игры, имеет немедленную награду 0, но также будет иметь некоторую будущую награду.`(state, action)`

Как мы будем представлять состояния и действия внутри программы на Python? «Состояние» игры Ним — это просто текущий размер всех стопок. Состояние, например, может быть , представляющим состояние с 1 объектом в куче 0, 1 объектом в куче 1, 3 объектами в куче 2 и 5 объектами в куче 3. "Действием" в игре Nim будет пара целых чисел , представляющая действие по взятию объектов из кучи . Таким образом, действие представляет собой действие «из кучи 3 убрать 5 предметов». Применение этого действия к состоянию приведет к новому состоянию (тому же состоянию, но с пустой стопкой 3).`[1, 1, 3, 5]``(i, j)``j``i``(3, 5)``[1, 1, 3, 5]``[1, 1, 3, 0]`

Напомним, что ключевая формула Q-обучения приведена ниже. Каждый раз, когда мы находимся в состоянии и выполняем действие, мы можем обновить Q-значение в соответствии с:`s``a``Q(s, a)`

```
Q(s, a) <- Q(s, a) + alpha * (new value estimate - old value estimate)

```

В приведенной выше формуле указан коэффициент обучения (насколько мы ценим новую информацию по сравнению с той, которую мы уже имеем). Он представляет собой сумму вознаграждения, полученного за текущее действие, и оценку всех будущих наград, которые получит игрок. Это просто существующее значение для . Применяя эту формулу каждый раз, когда наш ИИ совершает новое действие, со временем наш ИИ начнет узнавать, какие действия лучше в любом состоянии.`alpha``new value estimate``old value estimate``Q(s, a)`

[](#)## [Начало работы](https://cs50.harvard.edu/ai/projects/4/nim/#getting-started)* **Скачайте код дистрибутива с [https://cdn.cs50.net/ai/2023/x/projects/4/nim.zip](https://cdn.cs50.net/ai/2023/x/projects/4/nim.zip) и распакуйте его.

[](#)## [Понимание](https://cs50.harvard.edu/ai/projects/4/nim/#understanding)Во-первых, откройте . В этом файле определены два класса ( и ), а также две функции ( и ). , и уже реализованы за вас, в то время как вам осталось реализовать несколько функций.`nim.py``Nim``NimAI``train``play``Nim``train``play``NimAI`

Взгляните на класс, который определяет, как играть в игру Ним. Обратите внимание, что в каждой игре Nim необходимо отслеживать список стопок, текущего игрока (0 или 1) и победителя игры (если он существует). Функция возвращает набор всех доступных действий в состоянии. Например, возвращает множество , так как три возможных действия заключаются в том, чтобы взять либо 1 или 2 объекта из кучи 0, либо взять 1 объект из кучи 1.`Nim``__init__``available_actions``Nim.available_actions([2, 1, 0, 0])``{(0, 1), (1, 1), (0, 2)}`

Остальные функции используются для определения игрового процесса: функция определяет, кто является противником данного игрока, меняет текущего игрока на противоположного, а также выполняет действие над текущим состоянием и переключает текущего игрока на противоположного игрока.`other_player``switch_player``move`

Далее взгляните на класс, который определяет наш ИИ, который будет учиться играть в Ним. Обратите внимание, что в функции мы начинаем с пустого словаря. Словарь будет отслеживать все текущие Q-значения, изученные нашим искусственным интеллектом, сопоставляя пары с числовыми значениями. В качестве детали реализации, хотя мы обычно представляем их в виде списка, поскольку списки не могут использоваться в качестве ключей словаря Python, вместо этого мы будем использовать версию кортежа состояния при получении или установке значений в .`NimAI``__init__``self.q``self.q``(state, action)``state``self.q`

Например, если бы мы хотели установить Q-значение состояния и действия в значение , мы бы написали что-то вроде`[0, 0, 0, 2]``(3, 2)``-1`

```
self.q[(0, 0, 0, 2), (3, 2)] = -1

```

Заметьте также, что у каждого объекта есть значение and, которое будет использоваться для Q-обучения и для выбора действия соответственно.`NimAI``alpha``epsilon`

Функция написана для вас и принимает в качестве входного состояния действие в этом состоянии , результирующее состояние после выполнения этого действия и немедленное вознаграждение за выполнение этого действия . Затем функция выполняет Q-обучение, сначала получая текущее Q-значение для состояния и действия (путем вызова), определяя наилучшие возможные будущие вознаграждения (путем вызова ), а затем используя оба этих значения для обновления Q-значения (путем вызова ). Реализация этих трех функций оставлена на ваше усмотрение.`update``old_state``action``new_state``reward``get_q_value``best_future_reward``update_q_value`

Наконец, последняя функция, которая остается нереализованной, — это функция, которая выбирает действие для выполнения в заданном состоянии (либо с жадностью, либо с использованием алгоритма эпсилон-жадности).`choose_action`

Классы and в конечном итоге используются в функциях and. Функция обучает ИИ, запуская симулированные игры против самого себя, возвращая полностью обученный ИИ. Функция принимает обученный ИИ в качестве входных данных и позволяет игроку-человеку играть в игру «Ним» против ИИ.`Nim``NimAI``train``play``train``n``play`

[](#)## [Спецификация](https://cs50.harvard.edu/ai/projects/4/nim/#specification)Завершите реализацию , , , и в . Для каждой из этих функций каждый раз, когда функция принимает a в качестве входных данных, вы можете предположить, что это список целых чисел. Каждый раз, когда функция принимает an в качестве входных данных, вы можете предположить, что это целочисленная пара из кучи и ряда объектов .`get_q_value``update_q_value``best_future_reward``choose_action``nim.py``state``action``(i, j)``i``j`

Функция должна принимать в качестве входных данных a and и возвращать соответствующее Q-значение для этой пары состояние/действие.`get_q_value``state``action`

* **Напомним, что Q-значения хранятся в словаре. Ключи должны быть в виде пар, где кортеж всех размеров стопок по порядку, а кортеж представляет собой стопку и число.`self.q``self.q``(state, action)``state``action``(i, j)`
* **Если в , нет Q-значения для пары состояние/действие, то функция должна вернуть .`self.q``0`

Функция принимает состояние , действие , существующее значение Q , текущее вознаграждение и оценку будущих вознаграждений , а также обновляет Q-значение для пары состояние/действие в соответствии с формулой Q-обучения.`update_q_value``state``action``old_q``reward``future_rewards`

* **Напомним, что формула Q-обучения выглядит следующим образом: `Q(s, a) <- old value estimate + alpha * (new value estimate - old value estimate)`
* **Вспомните, что это скорость обучения, связанная с объектом.`alpha``NimAI`
* **Старая оценка значения — это просто существующее Q-значение для пары состояние/действие. Новая оценка стоимости должна быть суммой текущего вознаграждения и предполагаемого будущего вознаграждения.

Функция принимает a в качестве входных данных и возвращает наилучшее возможное вознаграждение за любое доступное действие в этом состоянии в соответствии с данными в .`best_future_reward``state``self.q`

* **Для любого действия, которое еще не существует для данного состояния, следует предположить, что оно имеет Q-значение, равное 0.`self.q`
* **Если в состоянии нет доступных действий, следует вернуть 0.

Функция должна принимать a в качестве входных данных (и, при необходимости, флаг для использования алгоритма epsilon-greedy) и возвращать доступное действие в этом состоянии.`choose_action``state``epsilon`

* **Если равно , ваша функция должна вести себя жадно и возвращать наилучшее возможное действие, доступное в этом состоянии (т. е. действие, которое имеет наибольшее Q-значение, используя 0, если Q-значение неизвестно).`epsilon``False`
* **Если есть , ваша функция должна вести себя в соответствии с эпсилон-жадным алгоритмом, выбирая случайное доступное действие с вероятностью и выбирая наилучшее доступное действие.`epsilon``True``self.epsilon`
* **Если несколько действий имеют одинаковое Q-значение, любой из этих вариантов является приемлемым возвращаемым значением.

Вы не должны изменять что-либо еще, кроме функций, которые спецификация призывает вас реализовать, хотя вы можете написать дополнительные функции и/или импортировать другие модули стандартной библиотеки Python. Вы также можете импортировать или, если вы знакомы с ними, но вы не должны использовать какие-либо другие сторонние модули Python. Вы можете внести изменения для тестирования самостоятельно.`nim.py``numpy``pandas``play.py`

[](#)## [Подсказки](https://cs50.harvard.edu/ai/projects/4/nim/#hints)* **Если это список, то может быть использован для преобразования в кортеж.`lst``tuple(lst)``lst`



