# Лекция 5 Нейронные сети

Нейронные сети в ИИ вдохновлены нейробиологией. В мозге **нейроны** — это клетки, которые соединены друг с другом, образуя сети. Каждый нейрон способен как получать, так и отправлять электрические сигналы. Как только электрический сигнал, полученный нейроном, превышает некоторый порог, нейрон активируется, отправляя свой электрический сигнал дальше.

**Искусственная нейронная сеть** — это математическая модель для обучения, вдохновленная биологическими нейронными сетями. Искусственные нейронные сети моделируют математические функции, которые отображают входы в выходы на основе структуры и параметров сети. В искусственных нейронных сетях структура сети формируется путем обучения на данных.

При реализации в ИИ аналогом каждого нейрона является **блок** (unit), который соединен с другими блоками. 
Например, как и в предыдущей лекции, ИИ может сопоставить два входа, x₁ и x₂, с тем, будет ли сегодня дождь или нет. В прошлой лекции мы предложили следующую форму для этой функции гипотезы: *h(x₁, x₂)* = *w₀ + w₁x₁ + w₂x₂,* где *w₁* и *w₂* — веса, изменяющие входные данные, а *w₀* — константа, также называемая **смещением**, изменяющая значение всего выражения.

## Функции активации

Чтобы использовать гипотетическую функцию для принятия решения о том, пойдет ли дождь, нам нужно создать какой-то порог на основе значения, которое она производит.

Один из способов сделать это — использовать **ступенчатую функцию**, которая дает 0 до достижения определенного порога и 1 после его достижения.


![image](./.assets/image-1761419208924.png)


Другой способ — использовать **логистическую функцию** (или сигмоиду), которая выдает на выходе любое действительное число от 0 до 1, выражая таким образом градуированную уверенность в своем суждении.


![image](./.assets/image-1761419214104.png)


Еще одна возможная функция — **Rectified Linear Unit (ReLU)**, или выпрямленный линейный блок, который позволяет выходу быть любым положительным значением. Если значение отрицательное, ReLU устанавливает его равным 0.


![image](./.assets/image-1761419221590.png)


Какую бы функцию мы ни выбрали, мы узнали на прошлой лекции, что входы изменяются весами в дополнение к смещению, и их сумма передается в функцию активации. Это остается верным и для простых нейронных сетей.



## Структура нейронной сети

Нейронную сеть можно рассматривать как представление приведенной выше идеи, где функция суммирует входы для получения выхода.


![image](./.assets/image-1761419231968.png)


Два белых блока слева — это входы, а блок справа — это выход. Входы соединены с выходом взвешенным ребром. Чтобы принять решение, выходной блок умножает входы на их веса в дополнение к смещению  (*w₀*) и использует функцию g для определения выхода.

Например, логический оператор **ИЛИ (Or)** может быть представлен как функция f со следующей таблицей истинности:


![image](./.assets/image-1761419297143.png)

Мы можем визуализировать эту функцию в виде нейронной сети. *x₁* — это одна входная единица, *а x₂* — другая входная единица. Они соединены с выходным блоком краем с весом 1. Затем единица вывода использует функцию *g(-1 + 1x₁ + 2x₂)* с пороговым значением 0 для вывода либо 0, либо 1 (false или true).

![image](./.assets/image-1761419317336.png)


Например, в случае, когда *x₁* = *x₂* = 0, сумма равна (-1). Это ниже порогового значения, поэтому функция *g* выведет 0. Однако, если одно или оба из *x₁* или *x₂* равны 1, то сумма всех входных данных будет либо 0, либо 1. Оба значения находятся на пороге или выше него, поэтому функция выведет 1.

Аналогичный процесс можно повторить с функцией And (где смещение будет равно (-2)). Более того, входы и выходы не обязательно должны быть различными. Аналогичный процесс может быть использован для учета влажности и давления воздуха в качестве входных данных и получения вероятности дождя на выходе. Или, в другом примере, входными данными могут быть деньги, потраченные на рекламу, и месяц, в котором они были потрачены, чтобы получить ожидаемый доход от продаж. Это число можно расширить на любое количество входов, умножив каждый вход *на x₁ ... xn* по весу *w₁ ... wn*, суммируя полученные значения и добавляя смещение *w₀*.

## Градиентный спуск

**Градиентный спуск** — это алгоритм для минимизации потерь (loss) при обучении нейронных сетей. Как упоминалось ранее, нейронная сеть способна извлекать знания о структуре самой сети из данных. В то время как до сих пор мы определяли различные веса сами, нейронные сети позволяют нам вычислять эти веса на основе обучающих данных. Для этого мы используем алгоритм градиентного спуска, который работает следующим образом:

1. Начать со **случайного выбора весов**. Это наша наивная отправная точка, где мы не знаем, какой вес следует придать каждому входу.
2. **Повторять:**

* **Вычислить градиент** на основе всех точек данных, который приведет к уменьшению потерь. В конечном счете, градиент — это вектор (последовательность чисел).
* **Обновить веса** в соответствии с градиентом.
3. **Вычислить градиент** на основе всех точек данных, который приведет к уменьшению потерь. В конечном счете, градиент — это вектор (последовательность чисел).
4. **Обновить веса** в соответствии с градиентом.

Проблема с таким алгоритмом заключается в том, что он требует вычисления градиента на основе **всех** точек данных, что вычислительно затратно. Существует несколько способов минимизировать эти затраты. Например, в **Стохастическом градиентном спуске (Stochastic Gradient Descent)** градиент вычисляется на основе одной случайно выбранной точки. Такой градиент может быть довольно неточным, что приводит к алгоритму **Пакетного градиентного спуска (Mini-Batch Gradient Descent)**, который вычисляет градиент на основе нескольких случайно выбранных точек, находя таким образом компромисс между вычислительными затратами и точностью. Как это часто бывает, ни одно из этих решений не является идеальным, и в разных ситуациях могут применяться разные решения.

Используя градиентный спуск, можно найти ответы на многие проблемы. Например, мы можем захотеть узнать больше, чем "пойдет ли сегодня дождь?". Мы можем использовать некоторые входы для генерации вероятностей различных типов погоды, а затем просто выбрать погоду, которая наиболее вероятна.


![image](./.assets/image-1761419387475.png)


Это можно сделать с любым количеством входов и выходов, где каждый вход соединен с каждым выходом, и где выходы представляют решения, которые мы можем принять. Обратите внимание, что в такого рода нейронных сетях выходы не соединены между собой. Это означает, что каждый выход и связанные с ним веса от всех входов можно рассматривать как отдельную нейронную сеть и, следовательно, обучать отдельно от остальных выходов.

До сих пор наши нейронные сети полагались на выходные блоки типа **перцептрон**. Это блоки, которые способны изучать только **линейную границу решения**, используя прямую линию для разделения данных. То есть, на основе линейного уравнения, перцептрон мог классифицировать вход как один тип или другой (например, картинка слева). Однако часто данные **не являются линейно разделимыми** (например, картинка справа). В этом случае мы обращаемся к многослойным нейронным сетям для нелинейного моделирования данных.


![image](./.assets/image-1761419399264.png)




## Многослойные нейронные сети

**Многослойная нейронная сеть** — это искусственная нейронная сеть с входным слоем, выходным слоем и как минимум одним **скрытым** слоем. В то время как мы предоставляем входы и выходы для обучения модели, мы, люди, не предоставляем никаких значений блокам внутри скрытых слоев. Каждый блок в первом скрытом слое получает взвешенное значение от каждого из блоков во входном слое, выполняет над ним какое-то действие и выводит значение. Каждое из этих значений взвешивается и далее распространяется на следующий слой, повторяя процесс до тех пор, пока не будет достигнут выходной слой. Через скрытые слои возможно моделировать нелинейные данные.


![image](./.assets/image-1761419430314.png)




## Обратное распространение ошибки (Backpropagation)

**Обратное распространение ошибки** — это основной алгоритм, используемый для обучения нейронных сетей со скрытыми слоями. Он делает это, начиная с ошибок в выходных блоках, вычисляя градиентный спуск для весов предыдущего слоя и повторяя процесс до тех пор, пока не будет достигнут входной слой. В псевдокоде мы можем описать алгоритм следующим образом:

1. **Вычислить ошибку** для выходного слоя.
2. **Для каждого слоя**, начиная с выходного и двигаясь внутрь к самому раннему скрытому слою:

* **Распространить ошибку** на один слой назад. Другими словами, текущий рассматриваемый слой отправляет ошибки предыдущему слою.
* **Обновить веса**.
3. **Распространить ошибку** на один слой назад. Другими словами, текущий рассматриваемый слой отправляет ошибки предыдущему слою.
4. **Обновить веса**.

Это можно расширить на любое количество скрытых слоев, создавая **глубокие нейронные сети**, которые представляют собой нейронные сети, имеющие более одного скрытого слоя.


![image](./.assets/image-1761419437832.png)


## Переобучение (Overfitting)

**Переобучение** — это опасность слишком точного моделирования обучающих данных, что приводит к неспособности обобщать на новые данные. Один из способов борьбы с переобучением — это **dropout** (прореживание). В этом методе мы временно удаляем случайно выбранные блоки на этапе обучения. Таким образом, мы пытаемся предотвратить чрезмерную зависимость от какого-либо одного блока в сети. На протяжении обучения нейронная сеть будет принимать разные формы, каждый раз отбрасывая одни блоки, а затем снова используя их:


![image](./.assets/image-1761419443993.png)


Обратите внимание, что после завершения обучения вся нейронная сеть будет использоваться снова.



## TensorFlow

Как это часто бывает в Python, множество библиотек уже имеют реализацию нейронных сетей с использованием алгоритма обратного распространения ошибки, и **TensorFlow** — одна из таких библиотек. Вы можете поэкспериментировать с нейронными сетями TensorFlow в этом **веб-приложении**, которое позволяет вам определять различные свойства сети и запускать ее, визуализируя результат. Теперь мы обратимся к примеру того, как мы можем использовать TensorFlow для выполнения задачи, которую мы обсуждали на прошлой лекции: отличать поддельные банкноты от настоящих.

```Python
import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split

```

Мы импортируем TensorFlow и называем его `tf` (чтобы сделать код короче).

```Python
# Считываем данные из файла
with open("banknotes.csv") as f:
    reader = csv.reader(f)
    next(reader)  # Пропускаем заголовок

    data = []
    for row in reader:
        data.append({
            "evidence": [float(cell) for cell in row[:4]],
            "label": 1 if row[4] == "0" else 0 
            # 0 - настоящая, 1 - поддельная (или наоборот, 
            # но здесь 0 маппится в 1, а другое (1) в 0)
        })

# Разделяем данные на обучающую и тестовую группы
evidence = [row["evidence"] for row in data]
labels = [row["label"] for row in data]
X_training, X_testing, y_training, y_testing = train_test_split(
    evidence, labels, test_size=0.4
)

```

Мы предоставляем CSV-данные модели. Наша работа часто заключается в том, чтобы привести данные к формату, который требует библиотека. Сложная часть фактического кодирования модели уже реализована за нас.

```Python
# Создаем нейронную сеть
model = tf.keras.models.Sequential()

```

Keras — это API, к которому обращаются различные алгоритмы машинного обучения. `Sequential` (последовательная) модель — это модель, где слои следуют друг за другом (как те, что мы видели до сих L1:F20пор).

```Python
# Добавляем скрытый слой с 8 блоками и активацией ReLU
model.add(tf.keras.layers.Dense(8, input_shape=(4,), activation="relu"))

```

`Dense` (полносвязный) слой — это слой, где каждый узел в текущем слое соединен со всеми узлами из предыдущего слоя. При создании наших скрытых слоев мы создаем один полносвязный слой с 8 блоками, каждый из которых имеет 4 входа (соответствует 4 признакам банкнот), используя функцию активации ReLU, упомянутую выше.

```Python
# Добавляем выходной слой с 1 блоком и активацией sigmoid
model.add(tf.keras.layers.Dense(1, activation="sigmoid"))

```

В нашем выходном слое мы хотим создать один полносвязный слой, который использует **сигмоидную** функцию активации — функцию активации, где выходным значением является число от 0 до 1 (что хорошо для бинарной классификации).

```Python
# Обучаем нейронную сеть
model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)
model.fit(X_training, y_training, epochs=20)

# Оцениваем, насколько хорошо работает модель
model.evaluate(X_testing, y_testing, verbose=2)

```

Наконец, мы **компилируем** модель, указывая, какой алгоритм должен ее оптимизировать (`optimizer="adam"`), какой тип функции потерь мы используем (`loss="binary_crossentropy"` - подходит для бинарной классификации), и как мы хотим измерять ее успех (в нашем случае, нас интересует `accuracy` - точность). Затем мы **обучаем** (`fit`) модель на обучающих данных в течение 20 повторений (эпох), а затем **оцениваем** (`evaluate`) ее на тестовых данных.



## Компьютерное зрение

**Компьютерное зрение** охватывает различные вычислительные методы для анализа и понимания цифровых изображений, и это часто достигается с помощью нейронных сетей. Например, компьютерное зрение используется, когда социальные сети применяют распознавание лиц для автоматической отметки людей на фотографиях. Другие примеры — распознавание рукописного ввода и беспилотные автомобили.

Изображения состоят из **пикселей**, а пиксели представлены тремя значениями в диапазоне от 0 до 255: одно для **красного**, одно для **зеленого** и одно для **синего**. Эти значения часто обозначаются аббревиатурой **RGB**. Мы можем использовать это для создания нейронной сети, где каждое значение цвета в каждом пикселе является входом, у нас есть несколько скрытых слоев, а выход — это некоторое количество блоков, которые говорят нам, что было показано на изображении. Однако у этого подхода есть несколько недостатков. Во-первых, разбивая изображение на пиксели и значения их цветов, мы не можем использовать структуру изображения как помощь. То есть, как люди, если мы видим часть лица, мы ожидаем увидеть и остальную часть лица, и это ускоряет вычисления. Мы хотим иметь возможность использовать подобное преимущество в наших нейронных сетях. Во-вторых, огромное количество входов очень велико, что означает, что нам придется вычислять много весов.



## Свертка изображений (Image Convolution)

**Свертка изображений** — это применение **фильтра**, который складывает значение каждого пикселя изображения с его соседями, взвешенными в соответствии с матрицей **ядра (kernel)**. Это изменяет изображение и может помочь нейронной сети обработать его.

Рассмотрим следующий пример:


![image](./.assets/image-1761419456919.png)


Ядро — это синяя матрица, а изображение — это большая матрица слева. Полученное отфильтрованное изображение — это маленькая матрица внизу справа. Чтобы отфильтровать изображение с помощью ядра, мы начинаем с пикселя со значением 20 в верхнем левом углу изображения (координаты 1,1). Затем мы умножим все значения вокруг него на соответствующее значение в ядре и сложим их (10*0 + 20*(-1) + 30*0 + 10*(-1) + 20*5 + 30*(-1) + 20*0 + 30*(-1) + 40*0), получая значение 10. Затем мы сделаем то же самое для пикселя справа (30), пикселя под первым (30) и пикселя справа от этого (40). Это создает отфильтрованное изображение со значениями, которые мы видим внизу справа.

Различные ядра могут выполнять разные задачи. Для **обнаружения краев** часто используется следующее ядро:


![image](./.assets/image-1761419469854.png)


Идея здесь в том, что когда пиксель похож на всех своих соседей, они должны компенсировать друг друга, давая значение 0. Поэтому, чем более похожи пиксели, тем темнее часть изображения, и чем они более различны, тем она светлее. Применение этого ядра к изображению (слева) приводит к изображению с выраженными краями (справа):


![image](./.assets/image-1761419475514.png)


Рассмотрим реализацию свертки изображений. Мы используем библиотеку **PIL** (Python Imaging Library), которая может выполнить большую часть тяжелой работы за нас.

```Python
import math
import sys
from PIL import Image, ImageFilter

# Проверяем корректность использования
if len(sys.argv) != 2:
    sys.exit("Usage: python filter.py filename")

# Открываем изображение
image = Image.open(sys.argv[1]).convert("RGB")

# Фильтруем изображение в соответствии с ядром обнаружения краев
filtered = image.filter(ImageFilter.Kernel(
    size=(3, 3),
    kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],
    scale=1
))

# Показываем полученное изображение
filtered.show()

```

Тем не менее, обработка изображения в нейронной сети вычислительно затратна из-за количества пикселей, которые служат входами для нейронной сети. Другой способ обойти это — **Пулинг (Pooling)**, или субдискретизация, при котором размер входа уменьшается путем выборки из областей на входе. Пиксели, находящиеся рядом друг с другом, принадлежат одной и той же области изображения, что означает, что они, вероятно, будут похожи. Поэтому мы можем взять один пиксель для представления целой области. Один из способов сделать это — **Max-Pooling** (максимальный пулинг), где выбранным пикселем является тот, у которого самое высокое значение из всех остальных в той же области. Например, если мы разделим левый квадрат (ниже) на четыре квадрата 2x2, путем max-pooling из этого входа мы получим маленький квадрат справа.


![image](./.assets/image-1761419483469.png)




## Сверточные нейронные сети (CNN)

**Сверточная нейронная сеть** — это нейронная сеть, которая использует свертку, обычно для анализа изображений. Она начинает с применения фильтров, которые могут помочь выделить некоторые особенности изображения, используя разные ядра. Эти фильтры можно улучшать так же, как и другие веса в нейронной сети, путем настройки их ядер на основе ошибки на выходе. Затем полученные изображения проходят через **пулинг**, после чего пиксели подаются в традиционную нейронную сеть в качестве входов (процесс, называемый **flattening** - выравнивание).


![image](./.assets/image-1761419491041.png)


Шаги свертки и пулинга могут повторяться несколько раз для извлечения дополнительных признаков и уменьшения размера входа в нейронную сеть. Одним из преимуществ этих процессов является то, что благодаря свертке и пулингу нейронная сеть становится менее чувствительной к вариациям. То есть, если одно и то же изображение снято с немного разных углов, вход для сверточной нейронной сети будет похожим, в то время как без свертки и пулинга входные данные от каждого изображения были бы совершенно разными.

В коде сверточная нейронная сеть не сильно отличается от традиционной нейронной сети. TensorFlow предлагает наборы данных для тестирования наших моделей. Мы будем использовать **MNIST**, который содержит черно-белые изображения рукописных цифр. Мы обучим нашу сверточную нейронную сеть распознавать цифры.

```Python
import sys
import tensorflow as tf

# Используем набор данных рукописных цифр MNIST
mnist = tf.keras.datasets.mnist

# Подготавливаем данные для обучения
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Нормализуем значения пикселей (от 0-255 к 0-1)
x_train, x_test = x_train / 255.0, x_test / 255.0

# Преобразуем метки в категориальный формат (one-hot encoding)
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

# Изменяем форму данных, чтобы добавить канал (для ч/б изображений = 1)
x_train = x_train.reshape(
    x_train.shape[0], x_train.shape[1], x_train.shape[2], 1
)
x_test = x_test.reshape(
    x_test.shape[0], x_test.shape[1], x_test.shape[2], 1
)

# Создаем сверточную нейронную сеть
model = tf.keras.models.Sequential([

    # Сверточный слой. Изучаем 32 фильтра (ядра) размером 3x3
    tf.keras.layers.Conv2D(
        32, (3, 3), activation="relu", input_shape=(28, 28, 1)
    ),

    # Слой Max-pooling, используя размер пула 2x2
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    # Выравниваем блоки (превращаем 2D в 1D)
    tf.keras.layers.Flatten(),

    # Добавляем скрытый полносвязный слой с dropout
    tf.keras.layers.Dense(128, activation="relu"),
    tf.keras.layers.Dropout(0.5),

    # Добавляем выходной слой с 10 блоками (для 10 цифр)
    # Активация softmax для получения вероятностей для каждого класса
    tf.keras.layers.Dense(10, activation="softmax")
])

# Обучаем нейронную сеть
model.compile(
    optimizer="adam",
    loss="categorical_crossentropy", # Потери для мультиклассовой классификации
    metrics=["accuracy"]
)
model.fit(x_train, y_train, epochs=10)

# Оцениваем производительность нейронной сети
model.evaluate(x_test,  y_test, verbose=2)
```

Поскольку обучение модели требует времени, мы можем сохранить уже обученную модель, чтобы использовать ее позже.

```Python
# Сохраняем модель в файл
if len(sys.argv) == 2:
    filename = sys.argv[1]
    model.save(filename)
    print(f"Модель сохранена в {filename}.")

```

Теперь, если мы запустим программу, которая получает на вход нарисованные от руки цифры, она сможет классифицировать и вывести цифру, используя модель. (Для реализации такой программы см. `recognition.py` в исходном коде этой лекции.)



## Рекуррентные нейронные сети (RNN)

**Нейронные сети прямого распространения (Feed-Forward Neural Networks)** — это тип нейронных сетей, который мы обсуждали до сих пор, где входные данные предоставляются сети, которая в конечном итоге производит некоторый выход. Диаграмму работы сетей прямого распространения можно увидеть ниже.


![image](./.assets/image-1761419500958.png)


В отличие от этого, **Рекуррентные нейронные сети (Recurrent Neural Networks)** состоят из нелинейной структуры, где сеть использует свой собственный выход в качестве входа. Например, **captionbot** от Microsoft способен описывать содержание изображения словами в предложении. Это отличается от классификации тем, что выход может иметь различную длину в зависимости от свойств изображения. В то время как сети прямого распространения неспособны изменять количество выходов, рекуррентные нейронные сети способны это делать благодаря своей структуре. В задаче создания подписей сеть будет обрабатывать вход для получения выхода, а затем продолжать обработку с этой точки, производя другой выход, и повторяя столько раз, сколько необходимо.


![image](./.assets/image-1761419506360.png)


Рекуррентные нейронные сети полезны в случаях, когда сеть имеет дело с **последовательностями**, а не с одним отдельным объектом. Выше нейронной сети нужно было произвести последовательность слов. Однако тот же принцип может быть применен к анализу видеофайлов, которые состоят из последовательности изображений, или в задачах перевода, где последовательность входов (слова на исходном языке) обрабатывается для получения последовательности выходов (слова на целевом языке).



