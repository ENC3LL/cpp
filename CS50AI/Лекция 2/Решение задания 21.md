# Решение задания 21

### PageRank (pagerank.py)

### Общее пояснение

Этот файл реализует алгоритм Google PageRank. Он определяет "важность" веб-страниц на основе ссылок между ними. В коде реализованы два метода:

1. **`sample_pagerank` (Метод сэмплирования):** Этот метод имитирует поведение "случайного серфера" (Random Surfer Model), который блуждает по сети. Он начинает со случайной страницы и затем `N` раз переходит по ссылкам в соответствии с `transition_model`. Страницы, на которых серфер оказывается чаще, получают более высокий ранг.
2. **`iterate_pagerank` (Итеративный метод):** Этот метод использует математическую формулу, которая определяет ранг страницы на основе рангов страниц, ссылающихся на нее. Формула применяется многократно (итеративно), пока значения PageRank не "сойдутся" (т.е. не перестанут меняться сильнее, чем на 0.001).

### Код

```py
import os
import random
import re
import sys

# DAMPING - Коэффициент затухания (обычно 0.85).
# Это вероятность того, что "случайный серфер" перейдет по ссылке
# на текущей странице.
DAMPING = 0.85

# SAMPLES - Количество выборок (шагов), которые сделает "случайный серфер"
# для оценки PageRank методом сэмплирования.
SAMPLES = 10000


def main():
    if len(sys.argv) != 2:
        sys.exit("Usage: python pagerank.py corpus")
    
    # crawl (сканировать) - эта функция читает все html-файлы в папке
    # и создает словарь "корпуса" страниц.
    corpus = crawl(sys.argv[1])
    
    # --- Метод Сэмплирования ---
    ranks = sample_pagerank(corpus, DAMPING, SAMPLES)
    print(f"PageRank Results from Sampling (n = {SAMPLES})")
    print("Результаты PageRank (метод сэмплирования):")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")

    # --- Итеративный Метод ---
    ranks = iterate_pagerank(corpus, DAMPING)
    print(f"PageRank Results from Iteration")
    print("Результаты PageRank (итеративный метод):")
    for page in sorted(ranks):
        print(f"  {page}: {ranks[page]:.4f}")


def crawl(directory):
    """
    Сканирует директорию с HTML-страницами и проверяет ссылки на другие страницы.
    Возвращает словарь, где каждый ключ - это страница, а значение -
    это МНОЖЕСТВО (set) всех других страниц в корпусе, на которые
    ссылается данная страница.
    """
    pages = dict()

    # Извлекаем все ссылки из HTML-файлов
    for filename in os.listdir(directory):
        if not filename.endswith(".html"):
            continue
        with open(os.path.join(directory, filename)) as f:
            contents = f.read()
            # Используем регулярное выражение для поиска всех тегов <a ... href="ССЫЛКА" ...>
            links = re.findall(r"<a\s+(?:[^>]*?)href=\"([^\"]*)\"", contents)
            # Убираем ссылки на саму себя
            pages[filename] = set(links) - {filename}

    # Оставляем только те ссылки, которые ведут на страницы ВНУТРИ нашего корпуса
    for filename in pages:
        pages[filename] = set(
            link for link in pages[filename]
            if link in pages
        )

    return pages


def transition_model(corpus, page, damping_factor):
    """
    Возвращает распределение вероятностей того, какую страницу посетить следующей,
    учитывая текущую страницу (`page`).

    С вероятностью `damping_factor` (0.85) выбирается случайная ссылка
    со страницы `page`.
    С вероятностью `1 - damping_factor` (0.15) выбирается случайная ссылка
    из ВСЕХ страниц в корпусе.
    """
    # Получаем все страницы в корпусе
    all_pages = set(corpus.keys())
    num_pages = len(all_pages)
    
    # Получаем ссылки с текущей страницы
    links = corpus[page]
    num_links = len(links)
    
    # Инициализируем словарь для вероятностей
    probabilities = {p: 0 for p in all_pages}
    
    if num_links == 0:
        # Особый случай: если у страницы нет ссылок.
        # Модель считает, что она ссылается на все страницы (включая себя)
        # с равной вероятностью.
        for p in all_pages:
            probabilities[p] = 1 / num_pages
    else:
        # Обычный случай:
        # 1. Вероятность случайного выбора ЛЮБОЙ страницы
        random_prob = (1 - damping_factor) / num_pages
        # 2. Вероятность перехода по конкретной ССЫЛКЕ с текущей страницы
        link_prob = damping_factor / num_links
        
        for p in all_pages:
            # Каждая страница получает "случайную" вероятность
            probabilities[p] += random_prob
            # Если на страницу `p` есть ссылка, добавляем "вероятность перехода по ссылке"
            if p in links:
                probabilities[p] += link_prob
                
    return probabilities


def sample_pagerank(corpus, damping_factor, n):
    """
    Возвращает значения PageRank для каждой страницы путем сэмплирования
    `n` страниц в соответствии с моделью перехода, начиная со случайной страницы.

    Возвращает словарь, где ключи - имена страниц, а значения -
    их оценочный PageRank (пропорция посещений).
    """
    # Инициализируем счетчики посещений для каждой страницы
    page_counts = {page: 0 for page in corpus}
    all_pages = list(corpus.keys())
    
    # Первый сэмпл: выбираем страницу случайным образом
    current_page = random.choice(all_pages)
    page_counts[current_page] += 1
    
    # Генерируем оставшиеся n-1 сэмплов
    for _ in range(n - 1):
        # Получаем модель перехода (вероятности) для текущей страницы
        trans_model = transition_model(corpus, current_page, damping_factor)
        
        # Извлекаем страницы и их вероятности
        pages = list(trans_model.keys())
        weights = list(trans_model.values())
        
        # Выбираем следующую страницу на основе взвешенных вероятностей
        # `random.choices` делает это за нас
        current_page = random.choices(pages, weights, k=1)[0]
        page_counts[current_page] += 1
        
    # Считаем итоговый PageRank: делим кол-во посещений на общее кол-во сэмплов `n`
    pageranks = {page: count / n for page, count in page_counts.items()}
    
    return pageranks


def iterate_pagerank(corpus, damping_factor):
    """
    Возвращает значения PageRank для каждой страницы путем итеративного обновления
    значений PageRank до достижения сходимости (изменения < 0.001).
    """
    num_pages = len(corpus)
    all_pages = set(corpus.keys())
    
    # Начальная инициализация: все страницы имеют ранг 1 / N
    pageranks = {page: 1 / num_pages for page in corpus}
    
    # (Оптимизация) Создаем словари для "обратных ссылок" и "количества ссылок"
    # `inverse_links[p]` = {страницы `i`, которые ссылаются на `p`}
    inverse_links = {page: set() for page in corpus}
    # `num_links[i]` = количество ссылок на странице `i`
    num_links = {}
    
    for page, links in corpus.items():
        if not links:
            # Если у страницы `page` нет ссылок, считаем, что она ссылается
            # на все N страниц.
            num_links[page] = num_pages
            for p in all_pages:
                inverse_links[p].add(page)
        else:
            # В обычном случае
            num_links[page] = len(links)
            for link in links:
                inverse_links[link].add(page) # `page` ссылается на `link`

    # Итеративно обновляем PageRank
    while True:
        new_pageranks = {}
        max_change = 0 # Отслеживаем максимальное изменение ранга в этой итерации
        
        # Рассчитываем новый ранг для КАЖДОЙ страницы `page_p`
        for page_p in corpus:
            
            # Первая часть формулы: (1 - d) / N
            pr_p = (1 - damping_factor) / num_pages
            
            # Вторая часть формулы: d * Σ (PR(i) / NumLinks(i))
            summation = 0
            # `page_i` - это все страницы, которые ссылаются на `page_p`
            for page_i in inverse_links[page_p]:
                summation += pageranks[page_i] / num_links[page_i]
                
            pr_p += damping_factor * summation
            new_pageranks[page_p] = pr_p
            
        # Проверяем сходимость: находим максимальную разницу
        # между старым и новым рангом
        max_change = max(abs(new_pageranks[p] - pageranks[p]) for p in corpus)
        
        # Обновляем ранги для следующей итерации
        pageranks = new_pageranks
        
        # Если ни один ранг не изменился > 0.001, мы "сошлись"
        if max_change < 0.001:
            break
            
    return pageranks


if __name__ == "__main__":
    main()
```