# LRU кэш

## Что такое LRU Cache?

**LRU Cache** расшифровывается как **"Least Recently Used" Cache**, то есть "кэш, вытесняющий наименее недавно используемый" элемент.

### Что такое кэш?
Представь, что у тебя есть очень медленный жесткий диск (или база данных), но очень быстрая оперативная память. Чтобы не "бегать" на диск за одними и теми же данными, ты хранишь самые *популярные* данные в быстрой памяти (в кэше).

### В чем проблема?
Быстрая память (кэш) всегда **маленькая**. Рано или поздно она заполнится. Когда ты захочешь положить в кэш новый элемент, а места нет, тебе придется что-то *выкинуть* (это называется "вытеснение" или "eviction").

### Какую стратегию вытеснения выбрать?
LRU — это одна из таких стратегий. Она основана на "принципе временнóй локальности" (temporal locality), который гласит: **если ты к чему-то недавно обратился, скорее всего, ты обратишься к этому снова в ближайшее время.**

И наоборот: **если ты чем-то не пользовался очень давно, скорее всего, оно тебе больше не понадобится.**

Поэтому, когда кэш заполнен, LRU-стратегия говорит: "Выкини тот элемент, который **использовался давнее всего**".

---

## Как он должен работать (API)

У LRU кэша есть два основных метода:

1. `get(key)` — Получить значение по ключу.
   - Если ключ есть в кэше:
     1. Мы "использовали" этот элемент. Значит, его нужно **пометить как "самый свежий"** (most recently used).
     2. Вернуть значение.
   - Если ключа нет:
     1. Вернуть "не найдено" (например, `-1`, `null` или `std::nullopt`).

2. `put(key, value)` — Положить (или обновить) значение по ключу.
   - Если ключ *уже есть* в кэше:
     1. Обновить его значение.
     2. Мы "использовали" этот элемент. **Пометить его как "самый свежий"**.
   - Если ключа *нет*:
     1. **Проверить, не полон ли кэш.**
     2. Если кэш полон:
        - Найти "самый старый" (least recently used) элемент.
        - **Вытеснить (удалить) его.**
     3. Добавить новый элемент `(key, value)`.
     4. **Пометить его как "самый свежий"**.

---

## Главная задача: Как сделать это *быстро*?

Все операции (`get` и `put`) в идеальном LRU кэше должны работать за **O(1)** — то есть за константное время, мгновенно, независимо от размера кэша.

Давай подумаем, какие структуры данных нам нужны:

1. Нам нужен **быстрый поиск, вставка и обновление** по ключу. Что у нас самое быстрое для этого? **Хэш-таблица** (в C++ это `std::unordered_map`). Она дает O(1) в среднем.
2. Нам нужно **отслеживать порядок "свежести"**. То есть иметь некий список, где с одного конца "самые свежие", а с другого — "самые старые".
3. Нам нужно **быстро перемещать** элемент в "самые свежие" (когда мы его `get`-аем) и **быстро удалять** "самый старый" (когда кэш полон).

Рассмотрим варианты:
- `std::vector`? Плохо. Переместить элемент из середины в начало — это O(N).
- Очередь (`std::queue`)? Плохо. Мы не можем "вытащить" элемент из середины и переставить его.

**Идеальный кандидат: Двусвязный список** (в C++ это `std::list`).
- Вставка в начало/конец: O(1)
- Удаление из начала/конца: O(1)
- **Удаление элемента из *середины* (если у нас есть на него указатель/итератор): O(1)**
- **Перемещение элемента (splice): O(1)**

## Решение: Комбинация двух структур!

Мы будем использовать **обе** структуры данных одновременно:

1. **`std::list<std::pair<Key, Value>>` (Двусвязный список)**
   - Он будет хранить *сами данные* (пары ключ-значение).
   - Мы договоримся: **`list.begin()` (начало списка) — это самый "свежий" (MRU)**.
   - **`list.back()` (конец списка) — это самый "старый" (LRU)**.
   - Зачем хранить `Key` в списке? Он понадобится нам при вытеснении, чтобы удалить ключ из хэш-таблицы.

2. **`std::unordered_map<Key, typename std::list<...>::iterator>` (Хэш-таблица)**
   - Это будет наш "индекс" для быстрого поиска.
   - Ключ (Key) будет ссылаться не на *значение* (Value), а на **итератор (указатель) на узел в `std::list`**, где лежат и ключ, и значение.

### Почему это работает:

**`get(key)`**:
1. Ищем `key` в `unordered_map`. O(1)
2. Не нашли? Возвращаем "не найдено". O(1)
3. Нашли? Мы получаем *итератор* на узел в `std::list`. O(1)
4. Берем `std::list` и **перемещаем (`splice`)** этот узел в самое начало списка (`items_list_.begin()`). Это O(1)
5. Возвращаем значение из узла. O(1)

**`put(key, value)`**:
1. Ищем `key` в `unordered_map`. O(1)
2. **Нашли (обновление)**:
   - Получаем итератор на узел в `std::list`. O(1)
   - Обновляем `value` в этом узле. O(1)
   - Перемещаем (`splice`) этот узел в начало списка. O(1)
3. **Не нашли (вставка)**:
   - Проверяем `if (list.size() == capacity)`. O(1)
   - **Если полон**:
     - Берем *последний* узел в списке (`list.back()`). Это LRU. O(1)
     - Берем из него `key` (вот зачем мы его там храним!). O(1)
     - Удаляем этот `key` из `unordered_map`. O(1)
     - Удаляем этот узел из списка (`list.pop_back()`). O(1)
   - **Вставляем новый элемент**:
     - Добавляем `(key, value)` в *начало* списка (`list.push_front()`). O(1)
     - Берем итератор на этот новый узел (`list.begin()`). O(1)
     - Вставляем в `unordered_map` пару `{key, list.begin()}`. O(1)

Все операции (в среднем) за O(1)!

---

## Полный код на C++ с пояснениями

Вот как это выглядит. Мы сделаем класс-шаблон, чтобы он работал для любых типов ключей и значений.

```cpp
#include <iostream>
#include <list>          // Наш двусвязный список для порядка "свежести"
#include <unordered_map> // Наша хэш-таблица для O(1) поиска
#include <optional>      // Для get(), чтобы красиво возвращать "не найдено"
#include <cstddef>       // Для size_t

// Мы будем использовать шаблоны, чтобы кэш мог хранить что угодно,
// не только int -> int.
template <typename Key, typename Value>
class LRUCache {
private:
    // Тип данных, который хранится в списке: пара (ключ, значение).
    using ListPair = std::pair<Key, Value>;
    
    // Тип итератора на узел в списке.
    using ListIt = typename std::list<ListPair>::iterator;

    size_t capacity_; // Максимальная вместимость кэша

    // 1. Двусвязный список для хранения (key, value) пар.
    //    begin() -- самый "свежий" (MRU)
    //    back()  -- самый "старый" (LRU)
    std::list<ListPair> items_list_;

    // 2. Хэш-таблица (индекс)
    //    Key -> Итератор на узел в items_list_
    std::unordered_map<Key, ListIt> cache_map_;

public:
    // Конструктор: задаем вместимость кэша
    LRUCache(size_t capacity) : capacity_(capacity) {
        // Убедимся, что вместимость не 0
        if (capacity_ == 0) {
            capacity_ = 1; 
        }
    }

    /**
     * @brief Получает значение по ключу.
     * @param key Ключ.
     * @return std::optional<Value>
     * - Содержит значение, если ключ найден.
     * - std::nullopt, если ключ не найден.
     */
    std::optional<Value> get(const Key& key) {
        // 1. Ищем ключ в хэш-таблице (O(1) в среднем)
        auto map_it = cache_map_.find(key);

        // 2. Если ключа нет - возвращаем "не найдено"
        if (map_it == cache_map_.end()) {
            return std::nullopt;
        }

        // 3. Ключ найден! Мы его "используем".
        //    Нужно переместить его в начало списка (сделать MRU).
        
        // map_it->second — это итератор на узел в items_list_
        // items_list_.splice() — это O(1) операция перемещения узла.
        // Мы говорим: "Переместить в items_list_.begin() 
        // из списка items_list_ узел, на который указывает map_it->second".
        items_list_.splice(items_list_.begin(), items_list_, map_it->second);

        // 4. Вернуть значение.
        //    map_it->second теперь указывает на узел в начале списка.
        //    ->second — это Value из пары (Key, Value).
        return map_it->second->second;
    }

    /**
     * @brief Добавляет или обновляет пару (ключ, значение).
     * @param key Ключ.
     * @param value Значение.
     */
    void put(const Key& key, const Value& value) {
        // 1. Ищем ключ в хэш-таблице (O(1) в среднем)
        auto map_it = cache_map_.find(key);

        // 2. Если ключ *уже существует* (обновление)
        if (map_it != cache_map_.end()) {
            // 2.1. Обновляем значение прямо в узле списка
            map_it->second->second = value;

            // 2.2. Мы "использовали" этот элемент, 
            //      перемещаем его в начало (делаем MRU).
            items_list_.splice(items_list_.begin(), items_list_, map_it->second);
            return;
        }

        // 3. Если ключа *нет* (новая вставка)
        
        // 3.1. Проверяем, не переполнен ли кэш
        if (items_list_.size() == capacity_) {
            // Кэш полон. Нужно вытеснить "самый старый" (LRU).
            // LRU-элемент лежит в *конце* списка (items_list_.back()).

            // 3.1.1. Получаем ключ LRU-элемента
            const Key& lru_key = items_list_.back().first;
            
            // 3.1.2. Удаляем этот ключ из хэш-таблицы (O(1) в среднем)
            cache_map_.erase(lru_key);

            // 3.1.3. Удаляем сам узел из списка (O(1))
            items_list_.pop_back();
        }

        // 3.2. Теперь в кэше есть место. 
        //      Добавляем новый элемент в *начало* списка (делаем MRU).
        items_list_.push_front({key, value});

        // 3.3. Добавляем запись о новом элементе в хэш-таблицу.
        //      items_list_.begin() — это итератор на только что созданный узел.
        cache_map_[key] = items_list_.begin();
    }
};

// --- Пример использования ---
int main() {
    // Создаем кэш вместимостью 2
    LRUCache<int, std::string> cache(2);

    cache.put(1, "Один"); // Кэш: [(1, "Один")]
    cache.put(2, "Два");  // Кэш: [(2, "Два"), (1, "Один")]

    // "Один" найден.
    if (auto val = cache.get(1)) {
        std::cout << "GET 1: " << *val << std::endl; 
    }
    // Кэш теперь: [(1, "Один"), (2, "Два")] (1 стал "самым свежим")

    cache.put(3, "Три");  // Кэш полон (размер 2).
                          // Вытесняем LRU-элемент (2, "Два").
                          // Кэш: [(3, "Три"), (1, "Один")]

    // "Два" не найден (был вытеснен).
    if (auto val = cache.get(2)) {
        std::cout << "GET 2: " << *val << std::endl;
    } else {
        std::cout << "GET 2: Не найдено" << std::endl;
    }

    cache.put(4, "Четыре"); // Кэш полон.
                           // Вытесняем LRU-элемент (1, "Один").
                           // Кэш: [(4, "Четыре"), (3, "Три")]

    // "Один" не найден.
    if (auto val = cache.get(1)) {
        std::cout << "GET 1: " << *val << std::endl;
    } else {
        std::cout << "GET 1: Не найдено" << std::endl;
    }

    // "Три" найден.
    if (auto val = cache.get(3)) {
        std::cout << "GET 3: " << *val << std::endl; 
    }
    // Кэш: [(3, "Три"), (4, "Четыре")] (3 стал "самым свежим")
    
    return 0;
}
```

### Вывод примера:

```
GET 1: Один
GET 2: Не найдено
GET 1: Не найдено
GET 3: Три
```

---

## Итог

- **Что такое LRU Cache?** Это кэш с политикой вытеснения: "выкидываем то, чем дольше всего не пользовались".
- **Зачем?** Чтобы в маленькой быстрой памяти хранить самые актуальные данные.
- **Как реализовать?** С помощью двух структур данных.
  1. **`std::unordered_map`**: Для быстрого (O(1)) поиска элемента по ключу. Хранит `Key -> Итератор_на_узел_в_списке`.
  2. **`std::list`**: Для отслеживания порядка "свежести" (O(1) на перемещение и удаление). Начало списка (MRU) - "свежие", конец (LRU) - "старые".
- **Магия C++:** Функция `std::list::splice()` — это сердце реализации. Она позволяет *перемещать* узел из одного места списка в другое (или даже в другой список) за O(1), не ломая итераторы и не выделяя новую память.
